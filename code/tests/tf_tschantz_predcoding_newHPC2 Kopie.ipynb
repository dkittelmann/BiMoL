{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiDe6B3CB1C2"
   },
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KtHYZ-I1BVie"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 18:28:39.568339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVV8cwB9Rthw"
   },
   "source": [
    "Helper and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WWsA42m-Rtnw"
   },
   "outputs": [],
   "source": [
    "def tf_scale_imgs(imgs, scale_factor):\n",
    "    return imgs * scale_factor + 0.5 * (1 - scale_factor) * tf.ones(imgs.shape)\n",
    "\n",
    "\n",
    "def tf_scale_labels(labels, scale_factor):\n",
    "    return labels * scale_factor + 0.5 * (1 - scale_factor) * tf.ones(labels.shape)\n",
    "\n",
    "\n",
    "def tf_f_inv(x, act_fn):\n",
    "    \"\"\" (activation_size, batch_size) \"\"\"\n",
    "    if act_fn == \"LINEAR\":\n",
    "        m = x\n",
    "    elif act_fn == \"TANH\":\n",
    "        num = tf.ones_like(x) + x\n",
    "        div = tf.ones_like(x) - x + 1e-7\n",
    "        m = 0.5 * tf.math.log(num / div)\n",
    "    elif act_fn == \"LOGSIG\":\n",
    "        div = tf.ones_like(x) - x + 1e-7\n",
    "        m = tf.math.log((x / div) + 1e-7)\n",
    "    else:\n",
    "        raise ValueError(f\"{act_fn} not supported\")\n",
    "    return m\n",
    "\n",
    "\n",
    "def boring_movie(x, y, n_steps=4):\n",
    "  \"\"\"Stacks a Tensor to create a 'time series' of repeating images.\"\"\"\n",
    "  x = tf.stack([x for _ in range(n_steps)], axis=1)  # make the \"boring\" movie of subsequently following images\n",
    "  y = tf.stack([y for _ in range(n_steps)], axis=1)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def random_omissions(x, y, omissions=0.5):\n",
    "  \"\"\"Randomly omits a fraction of the images in a batch.\"\"\" # why needed? -> not needed here \n",
    "  mask = tf.random.uniform(tf.shape(x)[:2], 0, 1)[:, :, None, None, None] > omissions\n",
    "  x = x * tf.cast(mask, dtype=tf.float32)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def img_preproc(x, y, dtype=tf.float32): # add: tf.image.resize(image, [28,28])\n",
    "  \"\"\"Cast input image to a certain tf dtype and normalize them between 0 and 1.\"\"\"\n",
    "  x = tf.cast(x, dtype) / 255.\n",
    "  #x = tf_scale_imgs(x, cf.img_scale)\n",
    "  #y = tf_scale_labels(y, cf.label_scale)\n",
    "  #x = tf_f_inv(x, \"TANH\")\n",
    "  #y = tf.one_hot(y, depth=10)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def flatten(x, y):\n",
    "  #flattens a video image series (or batch of images) to (n_batch, n_steps, 1) d.\n",
    "  shape = tf.shape(x)\n",
    "  if len(shape) == 5: # hack, determining if it's a video or not\n",
    "    x = tf.reshape(x, [shape[0], shape[1], -1])\n",
    "  elif len(shape) == 4:\n",
    "    x = tf.reshape(x, [shape[0], -1])\n",
    "  return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjMWCe7Mhct"
   },
   "source": [
    "build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vpg6GZ0QNwtg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDense(tf.keras.layers.Dense):\n",
    "    def call(self, inputs):\n",
    "        \"\"\"This works like a dense, except for the activation being called earlier.\"\"\"\n",
    "        # Apply the activation to the input first\n",
    "        activated_input = self.activation(inputs)\n",
    "        # Perform the matrix multiplication and add the bias\n",
    "        output = tf.matmul(activated_input, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "\n",
    "class PredictiveCodingNetwork(tf.keras.Sequential):\n",
    "    def __init__(self, layers, vars, beta, **kwargs):\n",
    "        \"\"\"Initialize a PredictiveCodingNetwork\"\"\"\n",
    "        super().__init__(layers, **kwargs)\n",
    "        self.vars = tf.convert_to_tensor(vars, dtype=tf.float32)\n",
    "        self.beta = beta\n",
    "\n",
    "    def call_with_states(self, x):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        x_list = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x_list.append(x)\n",
    "        return x_list\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        # do the stuff we do in train_epochs\n",
    "        outputs, errors = self.infer(x, y)\n",
    "        self.update_params(outputs, errors)\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        pred = self.call(x[:, 0])\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y[:, 0], pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def infer(self, x_batch, y_batch):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        errors = [None for _ in range(len(self.layers))]\n",
    "        f_x_arr = [None for _ in range(len(self.layers))]\n",
    "        f_x_deriv_arr = [None for _ in range(len(self.layers))]\n",
    "        shape = x_batch.shape\n",
    "        batch_size, n_steps = shape[0], shape[1]\n",
    "\n",
    "        for itr in range(n_steps):\n",
    "            # always update the current forward call\n",
    "            x_forward_cur = self.call_with_states(x_batch[:, itr])\n",
    "            x_forward_cur[-1] = y_batch[:, itr]\n",
    "            # if its the first itr, set x to the current forward call\n",
    "            if itr == 0:\n",
    "                x = x_forward_cur\n",
    "            if itr != 0:\n",
    "                # update g and x only for consecutive iterations\n",
    "                for l in range(1, len(self.layers)):\n",
    "                    g = tf.multiply(tf.matmul(errors[l], self.layers[l].kernel, transpose_b=True), f_x_deriv_arr[l])\n",
    "                    x[l] = x[l] + (x_forward_cur[l] - x_forward_prev[l]) + self.beta * (-errors[l-1] + g)\n",
    "            # update f_x etc for every iteration\n",
    "            for l in range(len(self.layers)):\n",
    "                f_x = self.layers[l].activation(x[l])\n",
    "                f_x_deriv_fn = self.get_activation_derivative(self.layers[l].activation)\n",
    "                f_x_deriv = f_x_deriv_fn(x[l])\n",
    "                f_x_arr[l] = f_x\n",
    "                f_x_deriv_arr[l] = f_x_deriv\n",
    "                errors[l] = (x[l + 1] - tf.matmul(f_x, self.layers[l].kernel) - self.layers[l].bias) / self.vars[l]\n",
    "            # fill the old forward pass with the current forward pass\n",
    "            x_forward_prev = x_forward_cur\n",
    "        return x, errors\n",
    "\n",
    "    def update_params(self, x, errors):\n",
    "        \"\"\"Update the model parameters.\"\"\"\n",
    "        batch_size = tf.cast(tf.shape(x[0])[0], tf.float32)\n",
    "        gradients = []\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            grad_w = self.vars[-1] * (1 / batch_size) * tf.matmul(tf.transpose(self.layers[l].activation(x[l])), errors[l])\n",
    "            grad_b = self.vars[-1] * (1 / batch_size) * tf.reduce_sum(errors[l], axis=0)\n",
    "            gradients.append((-grad_w, layer.kernel))\n",
    "            gradients.append((-grad_b, layer.bias))\n",
    "        self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        \"\"\"Return a function for the derivative of the given activation function.\"\"\"\n",
    "        activation_fn = tf.keras.activations.get(activation)\n",
    "        if activation_fn == tf.keras.activations.linear:\n",
    "            return lambda x: tf.ones_like(x)\n",
    "        elif activation_fn == tf.keras.activations.tanh:\n",
    "            return lambda x: 1 - tf.square(tf.nn.tanh(x))\n",
    "        elif activation_fn == tf.keras.activations.sigmoid:\n",
    "            return lambda x: tf.nn.sigmoid(x) * (1 - tf.nn.sigmoid(x))\n",
    "        else:\n",
    "            raise ValueError(f\"{activation} not supported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OARBmcvbf4mi"
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory /scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Leading/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     14\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m123\u001b[39m\n\u001b[0;32m---> 16\u001b[0m data_leading \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_dir_lead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_names_L\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#seed = seed\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m data_trailing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     27\u001b[0m     img_dir_trail, \n\u001b[1;32m     28\u001b[0m     label_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#seed = seed\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/BiMo_3.9/lib/python3.9/site-packages/keras/src/utils/image_dataset_utils.py:224\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/BiMo_3.9/lib/python3.9/site-packages/keras/src/utils/dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/BiMo_3.9/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Leading/"
     ]
    }
   ],
   "source": [
    "# Build dataset\n",
    "\n",
    "# 9 categories: Church, barn  beach  castle  cave  conference_room  forest  library  restaurant\n",
    "# leading: Barn, beach, library, restaurant, cave \n",
    "# trailing: Church, conference room, library, forest\n",
    "\n",
    "img_dir_lead = '/scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Leading/'\n",
    "img_dir_trail = '/scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Trailing/' \n",
    "class_names_L = ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
    "class_names_T = ['castle', 'Church', 'conference_room', 'forest'] # changed the order \n",
    "batch_size = 32\n",
    "image_size = (28,28)\n",
    "validation_split = 0.1\n",
    "seed = 123\n",
    "\n",
    "data_leading = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    #seed = seed\n",
    ")\n",
    "\n",
    "data_trailing = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_trail, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_T,\n",
    "    color_mode = 'rgb', \n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    #seed = seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1616 files belonging to 5 classes.\n",
      "Found 1616 files belonging to 4 classes.\n",
      "Leading imgs categories: ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
      "Trailing imgs categories: ['castle', 'Church', 'conference_room', 'forest']\n",
      "Category: 'barn'; Label: 0\n",
      "Category: 'beach'; Label: 1\n",
      "Category: 'cave'; Label: 2\n",
      "Category: 'library'; Label: 3\n",
      "Category: 'restaurant'; Label: 4\n",
      "Category: 'castle'; Label: 0\n",
      "Category: 'Church'; Label: 1\n",
      "Category: 'conference_room'; Label: 2\n",
      "Category: 'forest'; Label: 3\n",
      "Found 180 files belonging to 5 classes.\n",
      "Found 180 files belonging to 4 classes.\n",
      "Leading imgs categories: ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
      "Trailing imgs categories: ['castle', 'Church', 'conference_room', 'forest']\n",
      "Category: 'barn'; Label: 0\n",
      "Category: 'beach'; Label: 1\n",
      "Category: 'cave'; Label: 2\n",
      "Category: 'library'; Label: 3\n",
      "Category: 'restaurant'; Label: 4\n",
      "Category: 'castle'; Label: 0\n",
      "Category: 'Church'; Label: 1\n",
      "Category: 'conference_room'; Label: 2\n",
      "Category: 'forest'; Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a leading and trailing dataset with tensorflow\n",
    "\n",
    "img_dir_lead = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Leading/'\n",
    "img_dir_trail = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Trailing/'\n",
    "img_dir_test_lead = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Test/Test_Leading/'\n",
    "img_dir_test_trail = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Test/Test_Trailing/'\n",
    "class_names_L = ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
    "class_names_T = ['castle', 'Church', 'conference_room', 'forest'] # changed the order \n",
    "batch_size = None # adjust if needed, e.g., 32\n",
    "image_size = (28,28)\n",
    "validation_split = 0.1\n",
    "seed = 123\n",
    "\n",
    "data_leading = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    #shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "data_trailing = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_trail, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_T,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    image_size = image_size, \n",
    "    #shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "# Check class names and corresponding labels\n",
    "print(\"Leading imgs categories:\", data_leading.class_names)\n",
    "print(\"Trailing imgs categories:\", data_trailing.class_names)\n",
    "   \n",
    "class_names_dL = data_leading.class_names\n",
    "for index, class_name in enumerate(class_names_dL):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")\n",
    "        \n",
    "class_names_dT = data_trailing.class_names\n",
    "for index, class_name in enumerate(class_names_dT ):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")\n",
    "    \n",
    "\n",
    "test_leading = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_test_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    #shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "test_trailing = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_test_trail, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_T,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    image_size = image_size, \n",
    "    #shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "    \n",
    "print(\"Leading imgs categories:\", test_leading.class_names)\n",
    "print(\"Trailing imgs categories:\", test_trailing.class_names)\n",
    "   \n",
    "class_names_dL = data_leading.class_names\n",
    "for index, class_name in enumerate(class_names_dL):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")\n",
    "        \n",
    "class_names_dT = data_trailing.class_names\n",
    "for index, class_name in enumerate(class_names_dT ):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leading and trailing dataset with tensorflow\n",
    "\n",
    "img_dir_lead = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Leading/'\n",
    "img_dir_trail = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Trailing/'\n",
    "img_dir_test_lead = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Test/Test_Leading/'\n",
    "img_dir_test_trail = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Test/Test_Trailing/'\n",
    "class_names_L = ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
    "class_names_T = ['castle', 'Church', 'conference_room', 'forest'] # changed the order \n",
    "batch_size = None # adjust if needed, e.g., 32\n",
    "image_size = (28,28)\n",
    "validation_split = 0.1\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict that assigns the correct labels for each leading-trailing img pair\n",
    "\n",
    "\"\"\"\n",
    "L1 = barn = label 0 \n",
    "L2 = beach = label 1\n",
    "L3 = library = label 3\n",
    "L4 = restaurant = label 4 \n",
    "L5 = cave = label 2\n",
    "\n",
    "T6 = Church = label 1   \n",
    "T7 = conference room = label 2\n",
    "T8 = castle = label 0   \n",
    "T9 = forest = label 3\n",
    "\n",
    "MAPPING:\n",
    "\n",
    "L1 -> T6 = 0.75 -> (0,1) \n",
    "L1 -> T7 = 0.25 -> (0,2)\n",
    "L1 -> T8 = 0 -> (0,0)\n",
    "L1 -> T9 = 0 -> (0,3)\n",
    "\n",
    "L2 -> T6 = 0.75 -> (1,1) \n",
    "L2 -> T7 = 0.25 -> (1,2)\n",
    "L2 -> T8 = 0 -> (1,0)\n",
    "L2 -> T9 = 0 -> (1,3)\n",
    "\n",
    "L3 -> T6 = 0 -> (3,1) \n",
    "L3 -> T7 = 0 -> (3,2)\n",
    "L3 -> T8 = 0.5 -> (3,0)\n",
    "L3 -> T9 = 0.5 -> (3,3)\n",
    "\n",
    "L4 -> T6 = 0.25 -> (4,1) \n",
    "L4 -> T7 = 0.75 -> (4,2)\n",
    "L4 -> T8 = 0 -> (4,0)\n",
    "L4 -> T9 = 0 -> (4,3)\n",
    "\n",
    "L5 -> T6 = 0.25 -> (2,1) \n",
    "L5 -> T7 = 0.75 -> (2,2)\n",
    "L5 -> T8 = 0 -> (2,0)\n",
    "L5 -> T9 = 0 -> (2,3)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "label_dict = {\n",
    "    (0, 1): 0.0,\n",
    "    (0, 2): 0.0,\n",
    "    (0, 0): 0.25,\n",
    "    (0, 3): 0.75,\n",
    "    (1, 1): 0.0,\n",
    "    (1, 2): 0.0,\n",
    "    (1, 0): 0.25,\n",
    "    (1, 3): 0.75,\n",
    "    (3, 1): 0.0,\n",
    "    (3, 2): 0.0,\n",
    "    (3, 0): 0.25,\n",
    "    (3, 3): 0.75,\n",
    "    (4, 1): 0.0,\n",
    "    (4, 2): 0.0,\n",
    "    (4, 0): 0.25,\n",
    "    (4, 3): 0.75,\n",
    "    (2, 1): 0.0,\n",
    "    (2, 2): 0.0,\n",
    "    (2, 0): 0.5,\n",
    "    (2, 3): 0.5\n",
    "}\n",
    "\n",
    "\n",
    "def imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict): #data_t1, data_t2, label_dict, batch_size=None\n",
    "    \"\"\"\n",
    "    Function that stacks images and prepares labels for each batch.\n",
    "    + first preprocessing function: flattening \n",
    "    \"\"\"\n",
    "    \n",
    "    # go through each img and label in the leading & trailing batch \n",
    "    #img_t1, label_t1 = next(iter(data_t1)) # next(iter(data_t1.batch(batch_size)))\n",
    "    #img_t2, label_t2 = next(iter(data_t2))# next(iter(data_t2.batch(batch_size)))\n",
    "    \n",
    "    img_t1 = tf.cast(img_t1, dtype=tf.float32)\n",
    "    img_t2 = tf.cast(img_t2, dtype=tf.float32)\n",
    "    #print(type(img_t1))\n",
    "    \n",
    "    #print(\"Shape of img_t1:\", img_t1.shape)\n",
    "    #print(\"Shape of img_t2:\", img_t2.shape)\n",
    "    print(\"Shape of label_t1:\", label_t1.shape)\n",
    "    print(\"Shape of label_t2:\", label_t2.shape)\n",
    "    \n",
    "    \n",
    "    # stack imgs from both datatsets into sequence of img\n",
    "    x = tf.stack([img_t1, img_t1, img_t2, img_t2], axis = 1)\n",
    "    \n",
    "    #print(\"Type x:\", x.dtype)\n",
    "    #print(\"Shape of x after stacking:\", x.shape)\n",
    "    #print(\"imgs shape before labeling:\", img_t1.shape, img_t2.shape)\n",
    "    \n",
    "    #img_t1_flat = tf.reshape(img_t1, [tf.shape(img_t1)[0], -1])  # (batch_size, flattened_image_size)\n",
    "    #img_t2_flat = tf.reshape(img_t2, [tf.shape(img_t2)[0], -1])  # (batch_size, flattened_image_size)\n",
    "    #print(\"img flat:\", img_t1_flat.shape)\n",
    "    \n",
    "    # Stack the flattened images into sequences (keeping batch intact)\n",
    "    #x = tf.stack([img_t1_flat, img_t1_flat, img_t2_flat, img_t2_flat], axis=1)\n",
    "    #print(\"Shape of x after stacking:\", x.shape)\n",
    "    \n",
    "    #y = [label_dict.get((int(img_t1), int(img_t2)), 0) for img_t1, img_t2 in zip(label_t1, label_t2)]    \n",
    "    #print(\"Current labels:\", label_t1, label_t2)\n",
    "    #print(\"Type y before:\", y.dtype)\n",
    "    \n",
    "    y = []\n",
    "    for img1, img2 in zip(label_t1, label_t2):\n",
    "        # Create a label for each time step (4 labels in total for each sequence)\n",
    "        y.append([label_dict.get((int(img1), int(img2)), 0)] * 4)  # Repeat the label for each time step\n",
    "    \n",
    "    # Convert labels to a tensor and reshape to (batch_size, n_steps, 1)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    y = tf.reshape(y, (batch_size, 4, 1))  # Shape (batch_size, 4, 1)\n",
    "\n",
    "    return x, y\n",
    "    \n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    \n",
    "    if y.shape[0] != batch_size:\n",
    "        raise ValueError(f\"Y tensor size mismatch: expected {batch_size}, got {y.shape[0]}\")\n",
    "    \n",
    "    y = tf.reshape(y, (batch_size, 4, 1))\n",
    "    #print(\"Type y:\", type(y))\n",
    "    #print(\"Shape y :\", y.shape)\n",
    "    #print(\"Type of x:\", type(x), \"Shape of x:\", x.shape, \"dtype of x:\", x.dtype)\n",
    "    #print(\"Type of y:\", type(y), \"Shape of y:\", y.shape, \"dtype of y:\", y.dtype)\n",
    "    \n",
    "    #print(y)\n",
    "    #print(x)\n",
    "    \n",
    "    return x,y \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict that assigns the correct labels for each leading-trailing img pair\n",
    "\n",
    "\"\"\"\n",
    "L1 = barn = label 0 \n",
    "L2 = beach = label 1\n",
    "L3 = library = label 3\n",
    "L4 = restaurant = label 4 \n",
    "L5 = cave = label 2\n",
    "\n",
    "T6 = Church = label 1   \n",
    "T7 = conference room = label 2\n",
    "T8 = castle = label 0   \n",
    "T9 = forest = label 3\n",
    "\n",
    "MAPPING:\n",
    "\n",
    "L1 -> T6 = 0.75 -> (0,1) \n",
    "L1 -> T7 = 0.25 -> (0,2)\n",
    "L1 -> T8 = 0 -> (0,0)\n",
    "L1 -> T9 = 0 -> (0,3)\n",
    "\n",
    "L2 -> T6 = 0.75 -> (1,1) \n",
    "L2 -> T7 = 0.25 -> (1,2)\n",
    "L2 -> T8 = 0 -> (1,0)\n",
    "L2 -> T9 = 0 -> (1,3)\n",
    "\n",
    "L3 -> T6 = 0 -> (3,1) \n",
    "L3 -> T7 = 0 -> (3,2)\n",
    "L3 -> T8 = 0.5 -> (3,0)\n",
    "L3 -> T9 = 0.5 -> (3,3)\n",
    "\n",
    "L4 -> T6 = 0.25 -> (4,1) \n",
    "L4 -> T7 = 0.75 -> (4,2)\n",
    "L4 -> T8 = 0 -> (4,0)\n",
    "L4 -> T9 = 0 -> (4,3)\n",
    "\n",
    "L5 -> T6 = 0.25 -> (2,1) \n",
    "L5 -> T7 = 0.75 -> (2,2)\n",
    "L5 -> T8 = 0 -> (2,0)\n",
    "L5 -> T9 = 0 -> (2,3)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "label_dict = {\n",
    "    (0, 1): 0.0,\n",
    "    (0, 2): 0.0,\n",
    "    (0, 0): 0.25,\n",
    "    (0, 3): 0.75,\n",
    "    (1, 1): 0.0,\n",
    "    (1, 2): 0.0,\n",
    "    (1, 0): 0.25,\n",
    "    (1, 3): 0.75,\n",
    "    (3, 1): 0.0,\n",
    "    (3, 2): 0.0,\n",
    "    (3, 0): 0.25,\n",
    "    (3, 3): 0.75,\n",
    "    (4, 1): 0.0,\n",
    "    (4, 2): 0.0,\n",
    "    (4, 0): 0.25,\n",
    "    (4, 3): 0.75,\n",
    "    (2, 1): 0.0,\n",
    "    (2, 2): 0.0,\n",
    "    (2, 0): 0.5,\n",
    "    (2, 3): 0.5\n",
    "}\n",
    "\n",
    "\n",
    "def imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict): #data_t1, data_t2, label_dict, batch_size=None\n",
    "    \"\"\"\n",
    "    Function that stacks images and prepares labels for each batch.\n",
    "    + first preprocessing function: flattening \n",
    "    \"\"\"\n",
    "    \n",
    "    # go through each img and label in the leading & trailing batch \n",
    "    #img_t1, label_t1 = next(iter(data_t1)) # next(iter(data_t1.batch(batch_size)))\n",
    "    #img_t2, label_t2 = next(iter(data_t2))# next(iter(data_t2.batch(batch_size)))\n",
    "    \n",
    "    img_t1 = tf.cast(img_t1, dtype=tf.float32)\n",
    "    img_t2 = tf.cast(img_t2, dtype=tf.float32)\n",
    "    #print(type(img_t1))\n",
    "    \n",
    "    #print(\"Shape of img_t1:\", img_t1.shape)\n",
    "    #print(\"Shape of img_t2:\", img_t2.shape)\n",
    "    #print(\"Shape of label_t1:\", label_t1.shape)\n",
    "    #print(\"Shape of label_t2:\", label_t2.shape)\n",
    "    \n",
    "    \n",
    "    # stack imgs from both datatsets into sequence of img\n",
    "    x = tf.stack([img_t1, img_t1, img_t2, img_t2], axis = 0)\n",
    "    \n",
    "    #print(\"Type x:\", x.dtype)\n",
    "    #print(\"Shape of x after stacking:\", x.shape)\n",
    "    #print(\"imgs shape before labeling:\", img_t1.shape, img_t2.shape)\n",
    "    \n",
    "    #img_t1_flat = tf.reshape(img_t1, [tf.shape(img_t1)[0], -1])  # (batch_size, flattened_image_size)\n",
    "    #img_t2_flat = tf.reshape(img_t2, [tf.shape(img_t2)[0], -1])  # (batch_size, flattened_image_size)\n",
    "    #print(\"img flat:\", img_t1_flat.shape)\n",
    "    \n",
    "    # Stack the flattened images into sequences (keeping batch intact)\n",
    "    #x = tf.stack([img_t1_flat, img_t1_flat, img_t2_flat, img_t2_flat], axis=1)\n",
    "    #print(\"Shape of x after stacking:\", x.shape)\n",
    "    \n",
    "    #y = [label_dict.get((int(img_t1), int(img_t2)), 0) for img_t1, img_t2 in zip(label_t1, label_t2)]    \n",
    "    #print(\"Current labels:\", label_t1, label_t2)\n",
    "    #print(\"Type y before:\", y.dtype)\n",
    "    \n",
    "   # y = []\n",
    "    #for img1, img2 in zip(label_t1, label_t2):\n",
    "        # Create a label for each time step (4 labels in total for each sequence)\n",
    "    #    y.append([label_dict.get((int(img1), int(img2)), 0)] * 4)  # Repeat the label for each time step\n",
    "    \n",
    "    # Convert labels to a tensor and reshape to (batch_size, n_steps, 1)\n",
    "    #y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    #y = tf.reshape(y, (-1, 4, 1))  # Shape (batch_size, 4, 1)\n",
    "    #print(\"Shape y :\", y.shape)\n",
    "    \n",
    "    # Generate labels based on the sequence\n",
    "    #y_sequence = []\n",
    "    #label_value = label_dict.get((int(label_t1), int(label_t2)), 0)\n",
    "    #y_sequence = [[label_value]] * 4  # Create four labels, one for each step\n",
    "    #y = tf.convert_to_tensor(y_sequence, dtype=tf.float32)  # Shape (4, 1)\n",
    "\n",
    "\n",
    "    # Extract labels from scalars and generate the sequence\n",
    "    #label_value_t1 = label_dict.get(int(label_t1), 0)  # Convert scalar to int and look up in dictionary\n",
    "    #label_value_t2 = label_dict.get(int(label_t2), 0)  # Convert scalar to int and look up in dictionary\n",
    "    #print(\"original label1:\", label_t1)\n",
    "    #print(\"original label2:\", label_t1)\n",
    "    #print(\"labelvalue1:\", label_value_t1)\n",
    "    #print(\"labelvalue2:\",label_value_t2)\n",
    "    \n",
    "    \n",
    "    key_t1 = int(label_t1.numpy())\n",
    "    key_t2 = int(label_t2.numpy())\n",
    "    \n",
    "   # print(\"Current label pair:\", (key_t1, key_t2))\n",
    "    # print(\"All keys in label_dict:\", list(label_dict.keys()))  # Print all keys to check if they match\n",
    "    \n",
    "    # Check if the key exists\n",
    "    if (key_t1, key_t2) in label_dict:\n",
    "       label_value = label_dict[(key_t1, key_t2)]\n",
    "        #print(f\"Label value found: {label_value}\")\n",
    "    else:\n",
    "        print(f\"Label pair {(key_t1, key_t2)} not found in label_dict, defaulting to 0\")\n",
    "        label_value = 0  # or temporarily change this to another value to diagnose\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate the label sequence (label_t1, label_t1, label_t2, label_t2)\n",
    "    y_sequence = [[label_value], [label_value], [label_value], [label_value]]\n",
    "    y = tf.convert_to_tensor(y_sequence, dtype=tf.float32)  # Shape (4, 1)\n",
    "\n",
    "    # Print for verification\n",
    "   # print(\"Generated labels (y):\", y.numpy())\n",
    "    #print(\"Shape of y:\", y.shape)\n",
    "\n",
    "    return x, y\n",
    "    \n",
    "    #y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    \n",
    "    #if y.shape[0] != batch_size:\n",
    "     #   raise ValueError(f\"Y tensor size mismatch: expected {batch_size}, got {y.shape[0]}\")\n",
    "    \n",
    "    #y = tf.reshape(y, (batch_size, 4, 1))\n",
    "    #print(\"Type y:\", type(y))\n",
    "    #print(\"Shape y :\", y.shape)\n",
    "    #print(\"Type of x:\", type(x), \"Shape of x:\", x.shape, \"dtype of x:\", x.dtype)\n",
    "    #print(\"Type of y:\", type(y), \"Shape of y:\", y.shape, \"dtype of y:\", y.dtype)\n",
    "    \n",
    "    #print(y)\n",
    "    #print(x)\n",
    "    \n",
    "    #return x,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_FlatMapDataset element_spec=(TensorSpec(shape=(4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(img_dirt1, img_dirt2, class_namest1, class_namest2, label_dict, image_size = None, seed = None):\n",
    "        \n",
    "    # ADD Randomisation \n",
    "    # load datatsets -> dont batch \n",
    "    # shuffle both datasets \n",
    "    # put them together\n",
    "    # output: samples \n",
    "    \n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    data_t1 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        img_dirt1, \n",
    "        label_mode = 'int',\n",
    "        class_names= class_namest1,\n",
    "        batch_size = None,\n",
    "        color_mode = 'rgb',\n",
    "        image_size = image_size, \n",
    "        #shuffle = True, \n",
    "        seed = seed\n",
    "        )\n",
    "\n",
    "    data_t2 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        img_dirt2, \n",
    "        label_mode = 'int',\n",
    "        class_names= class_namest2,\n",
    "        batch_size = None,\n",
    "        color_mode = 'rgb', \n",
    "        image_size = image_size, \n",
    "        #shuffle = True, \n",
    "        seed = seed\n",
    "    )\n",
    "    \n",
    "    data_t1.shuffle(99999)\n",
    "    data_t2.shuffle(99999)\n",
    "    \n",
    "    # iterate through shuffled leading and trailing datasets\n",
    "    leading = iter(data_t1)\n",
    "    trailing = iter(data_t2) \n",
    "              \n",
    "    while True:\n",
    "        try:\n",
    "            # Retrieve single samples\n",
    "            img_t1, label_t1 = next(leading)\n",
    "            img_t2, label_t2 = next(trailing)\n",
    "            #print(\"test img shapes.\", img_t1.shape, img_t2.shape)\n",
    "            #print(\"Shape of label_t1:\", label_t1.shape)\n",
    "            #print(\"Shape of label_t2:\", label_t2.shape)\n",
    "\n",
    "            # Generate x, y pairs for single samples\n",
    "            x, y = imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict)\n",
    "            #print(\"test x:\", x.shape)\n",
    "            #print(\"test y:\", y.shape) \n",
    "            yield x, y\n",
    "            \n",
    "        except StopIteration:\n",
    "            # Break the loop if no more samples\n",
    "            break\n",
    "        \n",
    "        \n",
    "# Create new dataset \n",
    "n_steps = 4\n",
    "seed = 123\n",
    "#batch_size = 32\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_dataset(img_dir_lead, img_dir_trail, class_names_L, class_names_T, label_dict, image_size = (28,28), seed = seed),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(n_steps, 28, 28, 3), dtype=tf.float32),  # shape of x 2352; skip batch_size -> dataset.batch()\n",
    "        tf.TensorSpec(shape=(n_steps, 1), dtype=tf.float32)  # shape of y \n",
    "    )\n",
    ") \n",
    "\n",
    "print(dataset)\n",
    "  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1616 files belonging to 5 classes.\n",
      "Found 1616 files belonging to 4 classes.\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (4, 3)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.75\n",
      "Generated labels (y): [[0.75]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.75]]\n",
      "Current label pair: (0, 0)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.25\n",
      "Generated labels (y): [[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (4, 3)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.75\n",
      "Generated labels (y): [[0.75]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.75]]\n",
      "Current label pair: (4, 3)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.75\n",
      "Generated labels (y): [[0.75]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.75]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (4, 3)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.75\n",
      "Generated labels (y): [[0.75]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.75]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (1, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (0, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (1, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (1, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (0, 0)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.25\n",
      "Generated labels (y): [[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]]\n",
      "Current label pair: (0, 0)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.25\n",
      "Generated labels (y): [[0.25]\n",
      " [0.25]\n",
      " [0.25]\n",
      " [0.25]]\n",
      "Current label pair: (2, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (3, 2)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (2, 1)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.0\n",
      "Generated labels (y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Current label pair: (4, 3)\n",
      "All keys in label_dict: [(0, 1), (0, 2), (0, 0), (0, 3), (1, 1), (1, 2), (1, 0), (1, 3), (3, 1), (3, 2), (3, 0), (3, 3), (4, 1), (4, 2), (4, 0), (4, 3), (2, 1), (2, 2), (2, 0), (2, 3)]\n",
      "Label value found: 0.75\n",
      "Generated labels (y): [[0.75]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [0.75]]\n",
      "test x: (32, 4, 28, 28, 3)\n",
      "test y: (32, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.batch(32).take(1):\n",
    "    print(\"test x:\", x.shape)  # Should print (32, 4, 28, 28, 3)\n",
    "    print(\"test y:\", y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_FlatMapDataset element_spec=(TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 4, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset(data_t1, data_t2, label_dict, batch_size=None):\n",
    "    \n",
    "    # iterate through leading and trailing datasets and batch them accordingly \n",
    "    leading = iter(data_t1.batch(batch_size))\n",
    "    trailing = iter(data_t2.batch(batch_size)) # re-check -> sample batch\n",
    "    \n",
    "    # ADD Randomisation \n",
    "    # load datatsets -> dont batch \n",
    "    # shuffle both datasets \n",
    "    # put them together\n",
    "    # output: samples \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            img_t1, label_t1 = next(leading)\n",
    "            img_t2, label_t2 = next(trailing)\n",
    "            #print(\"test img shapes.\", img_t1.shape, img_t2.shape)\n",
    "\n",
    "            # Check if both current batches == batch_size, if not stop generating the dataset \n",
    "            if img_t1.shape[0] == batch_size and img_t2.shape[0] == batch_size:\n",
    "                \n",
    "                x, y = imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict, batch_size) # delete batch \n",
    "                #print(\"test x:\", x.shape)\n",
    "                #print(\"test y:\", y.shape)  \n",
    "                yield x, y\n",
    "            else:\n",
    "                print(f\"Skipping batch: leading batch size = {img_t1.shape[0]}, trailing batch size = {img_t2.shape[0]}\")\n",
    "                break\n",
    "        \n",
    "        except StopIteration:\n",
    "            # Break when there are no more samples\n",
    "            break\n",
    "        \n",
    "\n",
    "# Create new dataset \n",
    "n_steps = 4\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_dataset(data_leading, data_trailing, label_dict, batch_size=32),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, n_steps, 28, 28, 3), dtype=tf.float32),  # shape of x 2352; skip batch_size -> dataset.batch()\n",
    "        tf.TensorSpec(shape=(batch_size, n_steps, 1), dtype=tf.float32)  # shape of y \n",
    "    )\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "#for x, y in dataset:\n",
    "#    print(\"Shape of x:\", x.shape)\n",
    "#    print(\"Shape of y:\", y.shape)\n",
    "#    print(\"Type of x:\", x.dtype)\n",
    "#    print(\"Type of y:\", y.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(img_dirt1, img_dirt2, class_namest1, class_namest2, image_size = None, seed = None, label_dict):\n",
    "    \n",
    "    # iterate through leading and trailing datasets and batch them accordingly \n",
    "    leading = iter(data_t1)\n",
    "    trailing = iter(data_t2) # re-check -> sample batch\n",
    "    \n",
    "    # ADD Randomisation \n",
    "    # load datatsets -> dont batch \n",
    "    # shuffle both datasets \n",
    "    # put them together\n",
    "    # output: samples \n",
    "    \n",
    "    data_t1 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    batch_size = None,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    #shuffle = True, \n",
    "    seed = seed\n",
    "    )\n",
    "\n",
    "    data_t2 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        img_dir_trail, \n",
    "        label_mode = 'int',\n",
    "        class_names= class_names_T,\n",
    "        batch_size = None,\n",
    "        color_mode = 'rgb', \n",
    "        image_size = image_size, \n",
    "        #shuffle = True, \n",
    "        seed = seed\n",
    "    )\n",
    "    \n",
    "    data_t1.shuffle(99999)\n",
    "    data_t2.shuffle(99999)\n",
    "    \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            img_t1, label_t1 = next(leading)\n",
    "            img_t2, label_t2 = next(trailing)\n",
    "            #print(\"test img shapes.\", img_t1.shape, img_t2.shape)\n",
    "\n",
    "            # Check if both current batches == batch_size, if not stop generating the dataset \n",
    "            if img_t1.shape[0] == batch_size and img_t2.shape[0] == batch_size:\n",
    "                \n",
    "                x, y = imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict, batch_size) # delete batch \n",
    "                #print(\"test x:\", x.shape)\n",
    "                #print(\"test y:\", y.shape)  \n",
    "                yield x, y\n",
    "            else:\n",
    "                print(f\"Skipping batch: leading batch size = {img_t1.shape[0]}, trailing batch size = {img_t2.shape[0]}\")\n",
    "                break\n",
    "        \n",
    "        except StopIteration:\n",
    "            # Break when there are no more samples\n",
    "            break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training sequence accuracy\n",
    "\n",
    "def seq_accuracy(t[batch_size,4], o[batch_size, 4]):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        accuracies.append(tf.keras.metrics.accuracy())\n",
    "        \n",
    "    return accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch: leading batch size = 4, trailing batch size = 4\n",
      "Validation dataset size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:02:35.988916: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-10-04 14:02:38.175796: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches in train dataset: 50\n",
      "Total number of batches in val dataset: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:02:40.766395: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset_size = sum(1 for _ in dataset)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "print(f\"Val_dataset size: {val_size}\")\n",
    "\n",
    "# Split into train and validation datasets\n",
    "train_dataset = dataset.take(train_size)  # Take the first 90% for training\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "\n",
    "#print(train_dataset.element_spec)\n",
    "#print(val_dataset.element_spec)\n",
    "\n",
    "\n",
    "batch_count = 0\n",
    "for _ in train_dataset:\n",
    "    batch_count += 1\n",
    "print(f\"Total number of batches in train dataset: {batch_count}\")\n",
    "\n",
    "batch_count = 0\n",
    "for _ in val_dataset:\n",
    "    batch_count += 1\n",
    "print(f\"Total number of batches in val dataset: {batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of x: (32, 4, 28, 28, 3)\n",
      "Flattened shape of x: (32, 4, 2352)\n",
      "Shape of y (labels): (32, 4, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:02:44.381066: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in val_dataset.take(1):  # Taking one batch from your dataset\n",
    "    flattened_x, flattened_y = flatten(batch_x, batch_y)\n",
    "    print(f\"Original shape of x: {batch_x.shape}\")\n",
    "    print(f\"Flattened shape of x: {flattened_x.shape}\")\n",
    "    print(f\"Shape of y (labels): {flattened_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pZ1tUJ2MeVt"
   },
   "source": [
    "train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2NILa0_BMle",
    "outputId": "4f96d214-f8d9-447c-9c5a-027309864906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5890 - loss: 0.2096\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:02:57.555334: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.6421 - loss: 0.2307\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:03:01.454090: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.6544 - loss: 0.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 14:03:05.522770: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling PredictiveCodingNetwork.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"IteratorGetNext:0\", shape=(32, 4, 2352), dtype=float32). Expected shape (None, 2352), but input has incompatible shape (32, 4, 2352)\u001b[0m\n\nArguments received by PredictiveCodingNetwork.call():\n   inputs=tf.Tensor(shape=(32, 4, 2352), dtype=float32)\n   training=False\n   mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# evaluate accuracy and train model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#loss, accuracy = model.evaluate(val_dataset.map(img_preproc).map(flatten))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(f\"Accuracy before training: {accuracy}\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m99999\u001b[39m)\u001b[38;5;241m.\u001b[39mmap(img_preproc)\u001b[38;5;241m.\u001b[39mmap(flatten),\n\u001b[1;32m     19\u001b[0m           \u001b[38;5;66;03m#validation_data=val_dataset.batch(batch_size).map(img_preproc).map(flatten),\u001b[39;00m\n\u001b[1;32m     20\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_preproc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy after training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/BiMo_3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/BiMo_3.9/lib/python3.9/site-packages/keras/src/models/functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling PredictiveCodingNetwork.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"IteratorGetNext:0\", shape=(32, 4, 2352), dtype=float32). Expected shape (None, 2352), but input has incompatible shape (32, 4, 2352)\u001b[0m\n\nArguments received by PredictiveCodingNetwork.call():\n   inputs=tf.Tensor(shape=(32, 4, 2352), dtype=float32)\n   training=False\n   mask=None"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = PredictiveCodingNetwork([CustomDense(units=500, activation=\"tanh\"),\n",
    "                                 CustomDense(units=500, activation=\"tanh\"),\n",
    "                                 CustomDense(units=1, activation=\"sigmoid\")],\n",
    "                                vars=[1, 1, 1], # variances. This is super useless and in the code only the last variance is used\n",
    "                                beta=0.1)\n",
    "\n",
    "model.build([None, 2352])\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(),\n",
    "              metrics= seq_accuracy(),  #[\"accuracy\"],\n",
    "              loss=\"CategoricalCrossentropy\",  # This is just a sham loss we need so model.evaluate doesn't throw an error. We don't use it.\n",
    "              )\n",
    "\n",
    "# evaluate accuracy and train model\n",
    "#loss, accuracy = model.evaluate(val_dataset.map(img_preproc).map(flatten))\n",
    "#print(f\"Accuracy before training: {accuracy}\")\n",
    "\n",
    "for loop here \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_dataset(data_leading, data_trailing, label_dict, batch_size=32),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, n_steps, 28, 28, 3), dtype=tf.float32),  # shape of x 2352; skip batch_size -> dataset.batch()\n",
    "        tf.TensorSpec(shape=(batch_size, n_steps, 1), dtype=tf.float32)  # shape of y \n",
    "    )\n",
    ")\n",
    "\n",
    "model.fit(train_dataset.batch(32).shuffle(99999).map(img_preproc).map(flatten),\n",
    "          #validation_data=val_dataset.batch(batch_size).map(img_preproc).map(flatten),\n",
    "          epochs=3)\n",
    "\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(val_dataset.map(img_preproc).map(flatten))\n",
    "print(f\"Accuracy after training: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "BiMo_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
