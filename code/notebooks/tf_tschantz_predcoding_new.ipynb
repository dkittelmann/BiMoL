{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiDe6B3CB1C2"
   },
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KtHYZ-I1BVie"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 14:43:18.273334: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVV8cwB9Rthw"
   },
   "source": [
    "Helper and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WWsA42m-Rtnw"
   },
   "outputs": [],
   "source": [
    "def tf_scale_imgs(imgs, scale_factor):\n",
    "    return imgs * scale_factor + 0.5 * (1 - scale_factor) * tf.ones(imgs.shape)\n",
    "\n",
    "\n",
    "def tf_scale_labels(labels, scale_factor):\n",
    "    return labels * scale_factor + 0.5 * (1 - scale_factor) * tf.ones(labels.shape)\n",
    "\n",
    "\n",
    "def tf_f_inv(x, act_fn):\n",
    "    \"\"\" (activation_size, batch_size) \"\"\"\n",
    "    if act_fn == \"LINEAR\":\n",
    "        m = x\n",
    "    elif act_fn == \"TANH\":\n",
    "        num = tf.ones_like(x) + x\n",
    "        div = tf.ones_like(x) - x + 1e-7\n",
    "        m = 0.5 * tf.math.log(num / div)\n",
    "    elif act_fn == \"LOGSIG\":\n",
    "        div = tf.ones_like(x) - x + 1e-7\n",
    "        m = tf.math.log((x / div) + 1e-7)\n",
    "    else:\n",
    "        raise ValueError(f\"{act_fn} not supported\")\n",
    "    return m\n",
    "\n",
    "\n",
    "def boring_movie(x, y, n_steps=4):\n",
    "  \"\"\"Stacks a Tensor to create a 'time series' of repeating images.\"\"\"\n",
    "  x = tf.stack([x for _ in range(n_steps)], axis=1)  # make the \"boring\" movie of subsequently following images\n",
    "  y = tf.stack([y for _ in range(n_steps)], axis=1)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def random_omissions(x, y, omissions=0.5):\n",
    "  \"\"\"Randomly omits a fraction of the images in a batch.\"\"\" # why needed? -> not needed here \n",
    "  mask = tf.random.uniform(tf.shape(x)[:2], 0, 1)[:, :, None, None, None] > omissions\n",
    "  x = x * tf.cast(mask, dtype=tf.float32)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def img_preproc(x, y, dtype=tf.float32): # add: tf.image.resize(image, [28,28])\n",
    "  \"\"\"Cast input image to a certain tf dtype and normalize them between 0 and 1.\"\"\"\n",
    "  x = tf.cast(x, dtype) / 255.\n",
    "  #x = tf_scale_imgs(x, cf.img_scale)\n",
    "  #y = tf_scale_labels(y, cf.label_scale)\n",
    "  #x = tf_f_inv(x, \"TANH\")\n",
    "  #y = tf.one_hot(y, depth=10)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def flatten(x, y):\n",
    "  \"\"\"flattens a video image series (or batch of images) to (n_batch, n_steps, 1) d.\"\"\"\n",
    "  shape = tf.shape(x)\n",
    "  if len(shape) == 5: # hack, determining if it's a video or not\n",
    "    x = tf.reshape(x, [shape[0], shape[1], -1])\n",
    "  elif len(shape) == 4:\n",
    "    x = tf.reshape(x, [shape[0], -1])\n",
    "  return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_steps = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjMWCe7Mhct"
   },
   "source": [
    "build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vpg6GZ0QNwtg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDense(tf.keras.layers.Dense):\n",
    "    def call(self, inputs):\n",
    "        \"\"\"This works like a dense, except for the activation being called earlier.\"\"\"\n",
    "        # Apply the activation to the input first\n",
    "        activated_input = self.activation(inputs)\n",
    "        # Perform the matrix multiplication and add the bias\n",
    "        output = tf.matmul(activated_input, self.kernel)\n",
    "        if self.use_bias:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "\n",
    "class PredictiveCodingNetwork(tf.keras.Sequential):\n",
    "    def __init__(self, layers, vars, beta, **kwargs):\n",
    "        \"\"\"Initialize a PredictiveCodingNetwork\"\"\"\n",
    "        super().__init__(layers, **kwargs)\n",
    "        self.vars = tf.convert_to_tensor(vars, dtype=tf.float32)\n",
    "        self.beta = beta\n",
    "\n",
    "    def call_with_states(self, x):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        x_list = [x]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x_list.append(x)\n",
    "        return x_list\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        # do the stuff we do in train_epochs\n",
    "        outputs, errors = self.infer(x, y)\n",
    "        self.update_params(outputs, errors)\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        pred = self.call(x[:, 0])\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y[:, 0], pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def infer(self, x_batch, y_batch):\n",
    "        \"\"\"Note: while model call, call with states and model evaluate take\n",
    "        2D input, train_step and infer take stacked 3D inputs.\"\"\"\n",
    "        errors = [None for _ in range(len(self.layers))]\n",
    "        f_x_arr = [None for _ in range(len(self.layers))]\n",
    "        f_x_deriv_arr = [None for _ in range(len(self.layers))]\n",
    "        shape = x_batch.shape\n",
    "        batch_size, n_steps = shape[0], shape[1]\n",
    "\n",
    "        for itr in range(n_steps):\n",
    "            # always update the current forward call\n",
    "            x_forward_cur = self.call_with_states(x_batch[:, itr])\n",
    "            x_forward_cur[-1] = y_batch[:, itr]\n",
    "            # if its the first itr, set x to the current forward call\n",
    "            if itr == 0:\n",
    "                x = x_forward_cur\n",
    "            if itr != 0:\n",
    "                # update g and x only for consecutive iterations\n",
    "                for l in range(1, len(self.layers)):\n",
    "                    g = tf.multiply(tf.matmul(errors[l], self.layers[l].kernel, transpose_b=True), f_x_deriv_arr[l])\n",
    "                    x[l] = x[l] + (x_forward_cur[l] - x_forward_prev[l]) + self.beta * (-errors[l-1] + g)\n",
    "            # update f_x etc for every iteration\n",
    "            for l in range(len(self.layers)):\n",
    "                f_x = self.layers[l].activation(x[l])\n",
    "                f_x_deriv_fn = self.get_activation_derivative(self.layers[l].activation)\n",
    "                f_x_deriv = f_x_deriv_fn(x[l])\n",
    "                f_x_arr[l] = f_x\n",
    "                f_x_deriv_arr[l] = f_x_deriv\n",
    "                errors[l] = (x[l + 1] - tf.matmul(f_x, self.layers[l].kernel) - self.layers[l].bias) / self.vars[l]\n",
    "            # fill the old forward pass with the current forward pass\n",
    "            x_forward_prev = x_forward_cur\n",
    "        return x, errors\n",
    "\n",
    "    def update_params(self, x, errors):\n",
    "        \"\"\"Update the model parameters.\"\"\"\n",
    "        batch_size = tf.cast(tf.shape(x[0])[0], tf.float32)\n",
    "        gradients = []\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            grad_w = self.vars[-1] * (1 / batch_size) * tf.matmul(tf.transpose(self.layers[l].activation(x[l])), errors[l])\n",
    "            grad_b = self.vars[-1] * (1 / batch_size) * tf.reduce_sum(errors[l], axis=0)\n",
    "            gradients.append((-grad_w, layer.kernel))\n",
    "            gradients.append((-grad_b, layer.bias))\n",
    "        self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        \"\"\"Return a function for the derivative of the given activation function.\"\"\"\n",
    "        activation_fn = tf.keras.activations.get(activation)\n",
    "        if activation_fn == tf.keras.activations.linear:\n",
    "            return lambda x: tf.ones_like(x)\n",
    "        elif activation_fn == tf.keras.activations.tanh:\n",
    "            return lambda x: 1 - tf.square(tf.nn.tanh(x))\n",
    "        elif activation_fn == tf.keras.activations.sigmoid:\n",
    "            return lambda x: tf.nn.sigmoid(x) * (1 - tf.nn.sigmoid(x))\n",
    "        else:\n",
    "            raise ValueError(f\"{activation} not supported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OARBmcvbf4mi"
   },
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "ytr51C_PNNHH",
    "outputId": "3c1109e9-3039-442f-88e5-43301f8762fe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# define batch_size\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfds' is not defined"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "train_dataset = tfds.load(\"mnist\", split='train', as_supervised=True)\n",
    "val_dataset = tfds.load(\"mnist\", split='test', as_supervised=True)\n",
    "\n",
    "# define batch_size\n",
    "batch_size = 1024\n",
    "\n",
    "# give it one pass to test\n",
    "show_idx = 333\n",
    "for x, y in train_dataset.batch(batch_size).map(img_preproc).map(boring_movie).map(lambda x,y:random_omissions(x, y, omissions=0.5)).map(flatten).take(1):\n",
    "\n",
    "    # plot one sequence\n",
    "    fig, axes = plt.subplots(1, x.shape[1], figsize=(15, 4.1), sharey=True)\n",
    "    for idx in range(x.shape[1]):\n",
    "      axes[idx].imshow(tf.reshape(x[show_idx][idx], [28, 28, 1]), cmap=\"gray\")\n",
    "      axes[idx].get_xaxis().set_visible(False)\n",
    "      axes[idx].get_yaxis().set_visible(False)\n",
    "      axes[idx].set_title(f\"Step {idx}\")\n",
    "\n",
    "    # set title\n",
    "    plt.suptitle(f\"Showing input video sequence of sample {show_idx}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     15\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m123\u001b[39m\n\u001b[0;32m---> 17\u001b[0m data_leading \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     18\u001b[0m     img_dir_lead, \n\u001b[1;32m     19\u001b[0m     label_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     class_names\u001b[38;5;241m=\u001b[39m class_names_L,\n\u001b[1;32m     21\u001b[0m     color_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m     image_size \u001b[38;5;241m=\u001b[39m image_size, \n\u001b[1;32m     23\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#seed = seed\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m data_trailing \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     28\u001b[0m     img_dir_trail, \n\u001b[1;32m     29\u001b[0m     label_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#seed = seed\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Build dataset\n",
    "\n",
    "# 9 categories: Church, barn  beach  castle  cave  conference_room  forest  library  restaurant\n",
    "# leading: Barn, beach, library, restaurant, cave \n",
    "# trailing: Church, conference room, library, forest\n",
    "\n",
    "img_dir = '/scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/'\n",
    "img_dir_lead = '/scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Leading/'\n",
    "img_dir_trail = '/scratch/guetlid95/datasets/mcdermott_2024/Stimuli_2/Trailing/' \n",
    "class_names_L = ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
    "class_names_T = ['Church', 'castle', 'conference_room', 'forest']\n",
    "batch_size = 32\n",
    "image_size = (28,28)\n",
    "validation_split = 0.1\n",
    "seed = 123\n",
    "\n",
    "data_leading = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    #seed = seed\n",
    ")\n",
    "\n",
    "data_trailing = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_trail, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_T,\n",
    "    color_mode = 'rgb', \n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    #seed = seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1796 files belonging to 5 classes.\n",
      "Found 1796 files belonging to 4 classes.\n",
      "Leading imgs categories: ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
      "Trailing imgs categories: ['Church', 'castle', 'conference_room', 'forest']\n",
      "Category: 'barn'; Label: 0\n",
      "Category: 'beach'; Label: 1\n",
      "Category: 'cave'; Label: 2\n",
      "Category: 'library'; Label: 3\n",
      "Category: 'restaurant'; Label: 4\n",
      "Category: 'Church'; Label: 0\n",
      "Category: 'castle'; Label: 1\n",
      "Category: 'conference_room'; Label: 2\n",
      "Category: 'forest'; Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a leading and trailing dataset with tensorflow\n",
    "\n",
    "img_dir_lead = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Leading/'\n",
    "img_dir_trail = '/Users/denisekittelmann/Documents/Python/BiMoL/data/Trailing/'\n",
    "class_names_L = ['barn', 'beach', 'cave', 'library', 'restaurant']\n",
    "class_names_T = ['Church', 'castle', 'conference_room', 'forest']\n",
    "batch_size = None # adjust if needed, e.g., 32\n",
    "image_size = (28,28)\n",
    "validation_split = 0.1\n",
    "seed = 123\n",
    "\n",
    "data_leading = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_lead, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_L,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb',\n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "data_trailing = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    img_dir_trail, \n",
    "    label_mode = 'int',\n",
    "    class_names= class_names_T,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    image_size = image_size, \n",
    "    shuffle = True, \n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "# Check class names and corresponding labels\n",
    "print(\"Leading imgs categories:\", data_leading.class_names)\n",
    "print(\"Trailing imgs categories:\", data_trailing.class_names)\n",
    "   \n",
    "class_names_dL = data_leading.class_names\n",
    "for index, class_name in enumerate(class_names_dL):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")\n",
    "        \n",
    "class_names_dT = data_trailing.class_names\n",
    "for index, class_name in enumerate(class_names_dT ):\n",
    "    print(f\"Category: '{class_name}'; Label: {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict that assigns the correct labels for each leading-trailing img pair\n",
    "\n",
    "\"\"\"\n",
    "L1 = barn = label 0 \n",
    "L2 = beach = label 1\n",
    "L3 = library = label 3\n",
    "L4 = restaurant = label 4 \n",
    "L5 = cave = label 2\n",
    "\n",
    "T6 = Church = label 0 \n",
    "T7 = conference room = label 2\n",
    "T8 = castle = label 1\n",
    "T9 = forest = label 3\n",
    "\n",
    "MAPPING:\n",
    "\n",
    "L1 -> T6 = 0.75 -> (0,0)\n",
    "L1 -> T7 = 0.25 -> (0,2)\n",
    "L1 -> T8 = 0 -> (0,1)\n",
    "L1 -> T9 = 0 -> (0,3)\n",
    "\n",
    "L2 -> T6 = 0.75 -> (1,0)\n",
    "L2 -> T7 = 0.25 -> (1,2)\n",
    "L2 -> T8 = 0 -> (1,1)\n",
    "L2 -> T9 = 0 -> (1,3)\n",
    "\n",
    "L3 -> T6 = 0 -> (3,0)\n",
    "L3 -> T7 = 0 -> (3,2)\n",
    "L3 -> T8 = 0.5 -> (3,1)\n",
    "L3 -> T9 = 0.5 -> (3,3)\n",
    "\n",
    "L4 -> T6 = 0.25 -> (4,0)\n",
    "L4 -> T7 = 0.75 -> (4,2)\n",
    "L4 -> T8 = 0 -> (4,1)\n",
    "L4 -> T9 = 0 -> (4,3)\n",
    "\n",
    "L5 -> T6 = 0.25 -> (2,0)\n",
    "L5 -> T7 = 0.75 -> (2,2)\n",
    "L5 -> T8 = 0 -> (2,1)\n",
    "L5 -> T9 = 0 -> (2,3)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "label_dict = {\n",
    "    (0, 0): 0.0,\n",
    "    (0, 2): 0.0,\n",
    "    (0, 1): 0.25,\n",
    "    (0, 3): 0.75,\n",
    "    (1, 0): 0.0,\n",
    "    (1, 2): 0.0,\n",
    "    (1, 1): 0.25,\n",
    "    (1, 3): 0.75,\n",
    "    (3, 0): 0.0,\n",
    "    (3, 2): 0.0,\n",
    "    (3, 1): 0.25,\n",
    "    (3, 3): 0.75,\n",
    "    (4, 0): 0.0,\n",
    "    (4, 2): 0.0,\n",
    "    (4, 1): 0.25,\n",
    "    (4, 3): 0.75,\n",
    "    (2, 0): 0.0,\n",
    "    (2, 2): 0.0,\n",
    "    (2, 1): 0.5,\n",
    "    (2, 3): 0.5\n",
    "}\n",
    "\n",
    "\n",
    "def imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict, batch_size=None): #data_t1, data_t2, label_dict, batch_size=None\n",
    "    \"\"\"\n",
    "    Function that stacks images and prepares labels for each batch.\n",
    "    Now takes direct batch inputs instead of iterators.\n",
    "    \"\"\"\n",
    "    \n",
    "    # go through each img and label in the leading & trailing batch \n",
    "    #img_t1, label_t1 = next(iter(data_t1)) # next(iter(data_t1.batch(batch_size)))\n",
    "    #img_t2, label_t2 = next(iter(data_t2))# next(iter(data_t2.batch(batch_size)))\n",
    "    \n",
    "    img_t1 = tf.cast(img_t1, dtype=tf.float32)\n",
    "    img_t2 = tf.cast(img_t2, dtype=tf.float32)\n",
    "    #print(type(img_t1))\n",
    "    \n",
    "    #print(\"Shape of img_t1:\", img_t1.shape)\n",
    "    #print(\"Shape of img_t2:\", img_t2.shape)\n",
    "    #print(\"Shape of label_t1:\", label_t1.shape)\n",
    "    #print(\"Shape of label_t2:\", label_t2.shape)\n",
    "    \n",
    "    \n",
    "    # stack imgs from both datatsets into sequence of img\n",
    "    x = tf.stack([img_t1, img_t1, img_t2, img_t2], axis = 1)\n",
    "    #print(\"Type x:\", x.dtype)\n",
    "    #print(\"Shape of x after stacking:\", x.shape)\n",
    "    \n",
    "    y = [label_dict.get((int(img_t1), int(img_t2)), 0) for img_t1, img_t2 in zip(label_t1, label_t2)]    \n",
    "    #print(\"Current labels:\", label_t1, label_t2)\n",
    "    #print(\"Type y before:\", y.dtype)\n",
    "    \n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    \n",
    "    if y.shape[0] != batch_size:\n",
    "        raise ValueError(f\"Y tensor size mismatch: expected {batch_size}, got {y.shape[0]}\")\n",
    "    \n",
    "    y = tf.reshape(y, (batch_size, 1))\n",
    "    #print(\"Type y:\", type(y))\n",
    "    #print(\"Shape y :\", y.shape)\n",
    "    #print(\"Type of x:\", type(x), \"Shape of x:\", x.shape, \"dtype of x:\", x.dtype)\n",
    "    #print(\"Type of y:\", type(y), \"Shape of y:\", y.shape, \"dtype of y:\", y.dtype)\n",
    "    \n",
    "    #print(y)\n",
    "    #print(x)\n",
    "    \n",
    "    return x,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Create new dataset \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: generate_dataset(data_leading, data_trailing, label_dict, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m),\n\u001b[1;32m     32\u001b[0m     output_signature\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m     33\u001b[0m         tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32),  \u001b[38;5;66;03m# shape of x\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# shape of y \u001b[39;00m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataset:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_dataset(data_t1, data_t2, label_dict, batch_size=None):\n",
    "    \n",
    "    # iterate through leading and trailing datasets and batch them accordingly \n",
    "    leading = iter(data_t1.batch(batch_size))\n",
    "    trailing = iter(data_t2.batch(batch_size))\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            \n",
    "            img_t1, label_t1 = next(leading)\n",
    "            img_t2, label_t2 = next(trailing)\n",
    "\n",
    "            # Check if both current batches == batch_size, if not stop generating the dataset \n",
    "            if img_t1.shape[0] == batch_size and img_t2.shape[0] == batch_size:\n",
    "                \n",
    "                x, y = imgsequence(img_t1, img_t2, label_t1, label_t2, label_dict, batch_size)\n",
    "                #print(\"test x:\", x.shape)\n",
    "                #print(\"test y:\", y.shape)  \n",
    "                yield x, y\n",
    "            else:\n",
    "                print(f\"Skipping batch: leading batch size = {img_t1.shape[0]}, trailing batch size = {img_t2.shape[0]}\")\n",
    "                break\n",
    "        \n",
    "        except StopIteration:\n",
    "            # Break when there are no more samples\n",
    "            break\n",
    "        \n",
    "\n",
    "# Create new dataset \n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_dataset(data_leading, data_trailing, label_dict, batch_size=32),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32),  # shape of x\n",
    "        tf.TensorSpec(shape=(32, 1), dtype=tf.float32)  # shape of y \n",
    "    )\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "for x, y in dataset:\n",
    "    print(\"Shape of x:\", x.shape)\n",
    "    print(\"Shape of y:\", y.shape)\n",
    "    print(\"Type of x:\", x.dtype)\n",
    "    print(\"Type of y:\", y.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, validation_split=0.1, shuffle=True, seed=123):\n",
    "    \"\"\"Splits a dataset into training and validation datasets.\"\"\"\n",
    "    dataset_size = dataset.cardinality().numpy()  # Get the dataset size\n",
    "    val_size = int(validation_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=dataset_size, seed=seed)\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset: <_FlatMapDataset element_spec=(TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.float32, name=None))>\n",
      "Valset: <_DirectedInterleaveDataset element_spec=(TensorSpec(shape=(None, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_steps = 4\n",
    "train_leading, val_leading = split_data(data_leading, validation_split=0.1)\n",
    "train_trailing, val_trailing = split_data(data_trailing, validation_split=0.1)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_dataset(train_leading, train_trailing, label_dict, batch_size=batch_size),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, n_steps, 28, 28, 3), dtype=tf.float32),  # shape of x\n",
    "        tf.TensorSpec(shape=(batch_size, 1), dtype=tf.float32)  # shape of y \n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset_leading = val_leading.batch(batch_size)\n",
    "val_dataset_trailing = val_trailing.batch(batch_size)\n",
    "\n",
    "# Combine leading and trailing datasets for validation\n",
    "val_dataset = tf.data.experimental.sample_from_datasets([val_dataset_leading, val_dataset_trailing])\n",
    "\n",
    "print(\"Trainset:\", train_dataset)\n",
    "print(\"Valset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch: leading batch size = 4, trailing batch size = 4\n",
      "Validation dataset size: 6\n",
      "(TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.float32, name=None))\n",
      "(TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.float32, name=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 16:14:57.752873: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-09-27 16:14:59.514863: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches in train dataset: 50\n",
      "Total number of batches in val dataset: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 16:15:01.450268: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset_size = sum(1 for _ in dataset)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "print(f\"Validation dataset size: {val_size}\")\n",
    "\n",
    "# Split into train and validation datasets\n",
    "train_dataset = dataset.take(train_size)  # Take the first 90% for training\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "\n",
    "#print(train_dataset.element_spec)\n",
    "#print(val_dataset.element_spec)\n",
    "\n",
    "\n",
    "batch_count = 0\n",
    "for _ in train_dataset:\n",
    "    batch_count += 1\n",
    "print(f\"Total number of batches in train dataset: {batch_count}\")\n",
    "\n",
    "batch_count = 0\n",
    "for _ in val_dataset:\n",
    "    batch_count += 1\n",
    "print(f\"Total number of batches in val dataset: {batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pZ1tUJ2MeVt"
   },
   "source": [
    "train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2NILa0_BMle",
    "outputId": "4f96d214-f8d9-447c-9c5a-027309864906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724673876.088574 3559536 service.cc:146] XLA service 0x14a478004190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724673876.088603 3559536 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-08-26 14:04:36.406478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-26 14:04:37.005512: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.1396 - loss: 7.0209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724673877.877965 3559536 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.1404 - loss: 7.1597\n",
      "Accuracy before training: 0.1421000063419342\n",
      "Epoch 1/3\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.6300 - loss: 0.5808\n",
      "Epoch 2/3\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7755 - loss: 0.4985\n",
      "Epoch 3/3\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7872 - loss: 0.5350\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8369 - loss: 0.8267 \n",
      "Accuracy after training: 0.838699996471405\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = PredictiveCodingNetwork([CustomDense(units=500, activation=\"tanh\"),\n",
    "                                 CustomDense(units=500, activation=\"tanh\"),\n",
    "                                 CustomDense(units=10, activation=\"tanh\")],\n",
    "                                vars=[1, 1, 1], # variances. This is super useless and in the code only the last variance is used\n",
    "                                beta=0.1)\n",
    "\n",
    "model.build([None, 784])\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(),\n",
    "              metrics=[\"accuracy\"],\n",
    "              loss=\"CategoricalCrossentropy\",  # This is just a sham loss we need so model.evaluate doesn't throw an error. We don't use it.\n",
    "              )\n",
    "\n",
    "# evaluate accuracy and train model\n",
    "loss, accuracy = model.evaluate(val_dataset.map(img_preproc).map(flatten))\n",
    "print(f\"Accuracy before training: {accuracy}\")\n",
    "\n",
    "model.fit(train_dataset.shuffle(99999).map(img_preproc).map(flatten),\n",
    "          #validation_data=val_dataset.batch(batch_size).map(img_preproc).map(flatten),\n",
    "          epochs=3)\n",
    "\n",
    "loss, accuracy = model.evaluate(val_dataset.map(img_preproc).map(flatten))\n",
    "print(f\"Accuracy after training: {accuracy}\")\n",
    "\n",
    "# tensorboard -> history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch: leading batch size = 4, trailing batch size = 4\n",
      "Batch Images (x_batch): tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "Batch Images (y_batch): tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(0.75, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 10:31:50.397762: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tt = test_dataset.shuffle(99999).map(img_preproc).map(flatten)\n",
    "\n",
    "for x_batch, y_batch in tt.take(1):\n",
    "    input_shape = x_batch.shape\n",
    "    #print(\"Input Shape:\", input_shape)\n",
    "    print(\"Batch Images (x_batch):\", tf.reduce_min(x_batch), tf.reduce_max(x_batch))\n",
    "    print(\"Batch Images (y_batch):\", tf.reduce_min(y_batch), tf.reduce_max(y_batch))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(32, 4, 28, 28, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.float32, name=None))\n",
      "Skipping batch: leading batch size = 4, trailing batch size = 4\n",
      "Total number of batches: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 16:11:34.272440: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.element_spec)\n",
    "\n",
    "batch_count = 0\n",
    "for _ in test_dataset:\n",
    "    batch_count += 1\n",
    "print(f\"Total number of batches: {batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "BiMo_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
